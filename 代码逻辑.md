# 文本生成

### Data

数据文件中有多个样本记录，样本形式如下：通过 <sep> 分隔符将商品原始描述 **source** 和目标的摘要 **reference** 分隔开来。source 和 reference 是分词后的句子。

```python
女童 演出 服装 花童 公主 裙 婚纱 裙子 钢琴 礼服 裙 婚礼... <sep>... 凸显 时尚 品质 。 蓬蓬 坠地 裙摆 ， 更加 别致 优雅 。
```

### Train.py

**PairDataset 函数** 

```python
PairDataset
- data_path: 数据文件
- max_src_len: source 商品描述文字最大长度
- max_tgt_len: reference 摘要文字最大长度
- truncate_src: source 商品描述文字大于 max_src_len 最长长度时，是否截断操作
- truncate_tgt: source 摘要文字大于 max_tgt_len 最长长度时，是否截断操作
```

功能：

输入数据格式为：鸭 服饰 雪纺 连衣裙 女 夏季 2019 新款 ... <sep> ... 即便 是 黑色 也 毫不 沉闷 仙气 十足 。
返回数据对： (['鸭', '服饰', '雪纺', '连衣裙', '女', '夏季', '2019', '新款'... ], ['即便', '是', '黑色', '也', '毫不', '沉闷', '仙气', '十足'])

### Vocab.py

构建词典，通过 add_words 函数可以动态更新词典。该类提供如下功能：

- word2index：根据 word 得到对应的 index
- word2count：根据 word 得到该词的频率
- index2word：根据 index 得到对应的 word

该类还定义了 \_\_getitem__ 方法，实例化 Vocab 然后 vocab[5] 的方式取值

```Python
def __getitem__(self, item):
    """
    定义这个函数可以通过：实例化 Vocab 然后 vocab[5] 的方式取值
    """
    if type(item) is int:
        return self.index2word[item]
    return self.word2index.get(item, self.UNK)
```

### SampleDataset.py

```python
SampleDataset
- data_pair: 通过 PairDataset 获取到的数据对
- vocab: PairDataset 获取数据对时构建的词典，用来得到词到 index 的映射
```

输入： (['鸭', '服饰', '雪纺', '连衣裙', '女', '夏季', '2019', '新款'... ], ['即便', '是', '黑色', '也', '毫不', '沉闷', '仙气', '十足']) ...

```python
class SampleDataset(Dataset):
    """The class represents a sample set for training."""
    def __init__(self, data_pair, vocab):
        self.src_sents = [x[0] for x in data_pair]
        self.trg_sents = [x[1] for x in data_pair]
        self.vocab = vocab
        # Keep track of how many data points.
        self._len = len(data_pair)

    def __getitem__(self, index):
        x, oov = source2ids(self.src_sents[index], self.vocab)
        return {
            'x': [self.vocab.SOS] + x + [self.vocab.EOS],
            'OOV': oov,
            'len_OOV': len(oov),
            'y': [self.vocab.SOS] + abstract2ids(self.trg_sents[index], self.vocab, oov) + [self.vocab.EOS],
            'x_len': len(self.src_sents[index]),
            'y_len': len(self.trg_sents[index])
        }

    def __len__(self):
        return self._len
```

**source2ids 函数** 

输入为由 source 的词组成的 list：['鸭', '服饰', '雪纺', '连衣裙', '女', '夏季', '2019', '新款'... ]。该函数将 list 中的词映射成字典中对应的 id，如果词没有在字典中，则出现 oov 情况。解决方法为：比如该 list 中有三个 source oov 的词，第一个 source oov 的词在字典中没有对应的 id ，那么把它设置成 vocab_length + 0，第二个设置成 vocab_length + 1, 第三个设置成 vocab_length + 2。

最后返回将词转化成 id 的 list，和 oov 的词（词典中没出现过的词）

**abstract2ids 函数**

输入为由 reference 的词组成的 list：['即便', '是', '黑色', '也', '毫不', '沉闷', '仙气', '十足'... ]。该函数将 list 中的词映射成字典中对应的 id，只不过如果 list 中出现的词是 source 中 oov 的词，需要将 reference 中这个词设置成和 source 转 id 后的相同结果。比如 source 中的词 “雪纺” 是 source 中的第二个 oov 的词，那么如果对应的 reference 中也出现了这个词，应该把他们设置成同一个 id，即 vocab_length + 1

#### DataLoader - collate_fn

对每个 batch 的数据进行 padding，由于 SampleDataset.py 得到的数据长度是不一样的，需要对短的数据做 padding。并且 SampleDataset.py 得到的数据形式如下，需要将所有 batch 的 padding 后的 x 合并起来，以此类推。

每个 batch 是这样的：

```Python
 {
 'x': [SOS] + source_ids + [EOS],
 'OOV': oov,
 'len_OOV': len(oov),
 'y': [SOS] + reference_ids + [EOS],
 'x_len': source 词的长度,
 'y_len': reference 词的长度
 }
```

collate_fn 函数首先将所有 batch 的数据按照  x_len 进行排序。然后将 x 和 y 进行 padding，将 x padding 成 batch 中 x 最长的长度，将 y padding 层 batch 中 y 最长的长度。最终返回 x_padded, y_padded, x_len, y_len, OOV, len_OOV。而 x_len 是所有 batch 的 x_len 合并后的结果，y_len 也是所有 batch 的 y_len 合并的结果。

**Batch**

```python
for batch, data in enumerate(train_dataloader):
  x, y, x_len, y_len, oov, len_oovs = data
  """
  # 128: batch_size
  # print(x.shape) # torch.Size([128, 302])
  # print(y.shape) # torch.Size([128, 55])
  # print(x_len.shape) # torch.Size([128])
  # print(y_len.shape) # torch.Size([128])
  # print(len(oov)) # 128
  # print(len_oovs.shape) # torch.Size([128])

  # print(x[0]) # tensor([1, 5264, 3788 ... , 2])
  # print(y[0]) # tensor([1, 1542, 41, ... , 2, 0, 0])
  # print(x_len[0]) # tensor(302)
  # print(y_len[0]) # tensor(53)
  # print(oov[0]) # ['美背文', '变色功能', '70C']
  # print(len_oovs[0]) # tensor(3)
 """

```







